{
 "cells": [
  {
   "cell_type": "code",
   "id": "0cffc204-94ca-4021-ab7d-6ba25a6a83ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:02:19.143219Z",
     "start_time": "2024-12-22T19:02:19.125344Z"
    }
   },
   "source": [
    "def extract_section(ppt_path,toMatch):\n",
    "    presentation = Presentation(ppt_path)\n",
    "    slides_data = []\n",
    "\n",
    " \n",
    "    for slide_number, slide in enumerate(presentation.slides, start=1):\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_table:\n",
    "                table = shape.table\n",
    "                for idx, row in enumerate(table.rows):\n",
    "                    row_data = [cell.text.strip() for cell in row.cells]\n",
    "                    if toMatch in row_data:\n",
    "                        if (idx+1) < len(table.rows):\n",
    "                            row_data = [cell.text.strip() for cell in table.rows[idx+1].cells]\n",
    "                            slides_data.append(row_data)\n",
    "    return slides_data\n",
    "def extract_lines(data_list):\n",
    "    parts = []\n",
    "    for data in data_list:\n",
    "        combined_string = ''.join([str(item) for sublist in data for item in sublist])\n",
    "        lines = combined_string.replace(\"\\n\", '.').strip()\n",
    "        a=[cl.strip() for cl in lines.split('..') if cl.strip()]\n",
    "        parts.append(a)\n",
    "    parts = sum(parts,[])\n",
    "    return parts"
   ],
   "outputs": [],
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "id": "93856ed2-c6a3-4321-9ec4-0d860083b6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:02:21.563937Z",
     "start_time": "2024-12-22T19:02:21.545652Z"
    }
   },
   "source": [
    "from pptx import Presentation\n",
    "import pandas as pd\n",
    "#####the ppt was extracting each section as table\n",
    "def extract_tables(ppt_path,toMatch):\n",
    "    presentation = Presentation(ppt_path)\n",
    "    slides_data = []\n",
    "    for slide_number, slide in enumerate(presentation.slides, start=1):\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_table:\n",
    "                table = shape.table\n",
    "                for idx, row in enumerate(table.rows):\n",
    "                    row_data = [cell.text.strip() for cell in row.cells]\n",
    "                    if toMatch in row_data:\n",
    "                        while (idx) < len(table.rows):\n",
    "                            if(idx%(len(table.rows)) != 0):\n",
    "                                row_data = [cell.text.strip() for cell in table.rows[idx].cells]\n",
    "                                slides_data.append(row_data)\n",
    "                            idx=idx+1   \n",
    "    return slides_data\n",
    "def raid_table(data_list):\n",
    "    columns = ['RAID Type', 'RAID Description', 'Mitigation', 'Due Date', 'Owner', 'Status']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for data in data_list:\n",
    "        new_row = {\n",
    "            'RAID Type': data[0],\n",
    "            'RAID Description': data[1],\n",
    "            'Mitigation': data[2],\n",
    "            'Due Date': data[3],\n",
    "            'Owner': data[4],\n",
    "            'Status': data[5]\n",
    "        }\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def milestone_table(data_list):\n",
    "    columns = ['Milestone', 'Due Date',  'Status']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for data in data_list:\n",
    "        new_row = {\n",
    "            'Milestone': data[0],\n",
    "            'Due Date': data[1],\n",
    "            'Status': data[2]\n",
    "        }\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 150
  },
  {
   "cell_type": "markdown",
   "id": "2178949b-299d-4236-ad8b-5e79e7317b7d",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T17:20:09.580019Z",
     "start_time": "2024-12-22T17:20:06.706770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####Using Cosine Similarity Using Sentence-BERT Model\n",
    "#!pip install sentence-transformers\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "def calcosim(reference, candidate):\n",
    "    ref = model.encode(reference, convert_to_tensor=True)\n",
    "    cad = model.encode(candidate, convert_to_tensor=True)\n",
    "    score = util.pytorch_cos_sim(ref, cad).item()\n",
    "    return score\n"
   ],
   "id": "f4bf440a662f6275",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:23:18.792426Z",
     "start_time": "2024-12-22T19:23:18.782147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ppt1_path = r\"C:\\Users\\shsarma\\Desktop\\GenAI-AscendAIP\\WSR\\Samples With GT\\Sample 7\\Gt.pptx\"\n",
    "ppt2_path = r\"C:\\Users\\shsarma\\Desktop\\GenAI-AscendAIP\\WSR\\Samples With GT\\Sample 7\\Gen.pptx\""
   ],
   "id": "e7e9c2a0aa80a0ac",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:58:26.282965Z",
     "start_time": "2024-12-22T18:58:25.884900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "toMatch = \"Key Activities Accomplished This Week\"\n",
    "list1 = extract_lines(extract_section(ppt1_path, toMatch))\n",
    "list2 = extract_lines(extract_section(ppt2_path, toMatch))\n",
    "print(list1)\n",
    "print('-----------------------------------------')\n",
    "print(list2)\n",
    "ref = model.encode(list1, convert_to_tensor=True)\n",
    "cad = model.encode(list2, convert_to_tensor=True)\n",
    "\n",
    "cosine_sim_matrix = util.pytorch_cos_sim(ref, cad)\n",
    "cosine_sim_matrix = cosine_sim_matrix.cpu().numpy()\n",
    "max_sim = np.max(cosine_sim_matrix, axis=1)\n",
    "#bert_score = np.mean(cosine_sim_matrix)*100\n",
    "bert_score1 = np.mean(max_sim)*100\n",
    "#print (f\"Cosine Similarity Score for the Section - {toMatch}: {bert_score: .2f}%\")\n",
    "print (f\"Cosine Similarity Score for the Section - {toMatch}: {bert_score1: .2f}%\")\n",
    "\n"
   ],
   "id": "4b31b83c673a9672",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Input files were created and MOM for same is generated.']\n",
      "-----------------------------------------\n",
      "['• Six projects were created and checked for user accessibility', '• The team divided the work among themselves, with Varsha and Alicia working on one WSR and one RAID, and Neha, Akhil, and Amartya working on two of them', '• The team created manual outputs for the input files, including MOM, RAID, and WSR, using the same format as the tool-generated output', '• The team worked on generating manual outputs for the existing recording in SharePoint', '• The team committed to providing three sets of manually generated data for three MOMs by 2:00 PM.', '• The team provided at least seven sets of data to Angola, including 3 MOMs, 3 RAIDs, and 3 WSRs', '• The team created a demo ICS invite for the current meeting and uploaded the input files', '• The team shared and discussed the input details in a SharePoint file', '• An email was sent by Angola for the creation of some test data, which was followed up on', '• The team committed to reconnecting at 2:00 PM to discuss progress and next steps.']\n",
      "Cosine Similarity Score for the Section - Key Activities Accomplished This Week:  58.21%\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:20:18.676074Z",
     "start_time": "2024-12-22T19:20:18.237455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "toMatch=\"Key Activities Planned for Next Week\" or \"Key Activities Planned For Next Week\"\n",
    "\n",
    "list1 = extract_lines(extract_section(ppt1_path, toMatch))\n",
    "list2 = extract_lines(extract_section(ppt2_path, toMatch))\n",
    "print(list1)\n",
    "print('-----------------------------------------')\n",
    "print(list2)\n",
    "ref = model.encode(list1, convert_to_tensor=True)\n",
    "cad = model.encode(list2, convert_to_tensor=True)\n",
    "\n",
    "cosine_sim_matrix = util.pytorch_cos_sim(ref, cad)\n",
    "cosine_sim_matrix = cosine_sim_matrix.cpu().numpy()\n",
    "max_sim = np.max(cosine_sim_matrix, axis=1)\n",
    "#bert_score = np.mean(cosine_sim_matrix)*100\n",
    "bert_score1 = np.mean(max_sim)*100\n",
    "#print (f\"Cosine Similarity Score for the Section - {toMatch}: {bert_score: .2f}%\")\n",
    "print (f\"Cosine Similarity Score for the Section - {toMatch}: {bert_score1: .2f}%\")"
   ],
   "id": "b22354c05a922f42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Akhil to check with Meru Shah for project visibility on AIP Since project was not visible to Sagar', 'Backend Team need to check how accurate the output generated by AIP and output written by team is', 'Everyone to check if they are able to view Projects in Production environment', 'Team need to create data manually for MOM , WSR and Raid with the certain input files. Similar format to that of AIP generated output needs to used for the manually created data', 'Huge Amount of data is needed at least 7 set of data . We need to provide 3 set of data ( 3 MOM , 3 Raid and 3 WSR)', 'By today 2 PM team need to provide at least 2 sets of output by every single member of the team', 'In AIP Internal teams channel, we have input and output files stored . Input folder containing .ics and .vtt files', 'Alisha and Varsha collectively need to work on existing recording for WSR and RIAD', 'Amartya , Sagar , Neha and Akhil need to work on Manual generation of MOM , RAID and WSR', 'Akhil to work on MOM , Neha to work on WSR and Amartya will be working on RAID', 'By 2 PM we need to provide the outputs.']\n",
      "-----------------------------------------\n",
      "['• Sagar will communicate with Nirusha regarding project visibility issues', '• The team will reconvene at 2:00 PM to review progress and address any issues', '• All team members are to manually generate three sets of data for one WSR by 2:00 PM', '• Varsha and Alicia are to work on one WSR, using the input documents already uploaded in SharePoint', '• Neha, Akhil, and Amartya are to work on two sets of data, one of which will be provided by Varsha.', '• A third input file will be created from the recording of the current meeting, with Varsha creating a demo ICS invite and Alicia generating a VTT file.']\n",
      "Cosine Similarity Score for the Section - Key Activities Planned for Next Week:  54.37%\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:13:38.423225Z",
     "start_time": "2024-12-22T18:13:38.415590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def eval_df(df_gt, df_gen, bertCol, bertCol1 = None, exactCol = [], sim_thres = None):\n",
    "\n",
    "    R_t = len(df_gt)\n",
    "    R_n = len(df_gen)\n",
    "    R_m = 0\n",
    "    unmatched_gen = set(df_gen.index)\n",
    "    used_gt = set()\n",
    "    for idx_gen, row_gen in df_gen.iterrows():\n",
    "        max_sim = 0\n",
    "        best_mat_idx = None\n",
    "        for idx_gt, row_gt in df_gt[~df_gt.index.isin(used_gt)].iterrows():\n",
    "            sim = calcosim(row_gt[bertCol].item(), row_gen[bertCol].item())\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                best_mat_idx = idx_gt\n",
    "        if max_sim >= sim_thres:\n",
    "            if bertCol1:\n",
    "                nam = calcosim(row_gen['Owner'], df_gt.loc[best_mat_idx, 'Owner'])\n",
    "                nam1 = calcosim(row_gen[bertCol1].item(), df_gt.loc[best_mat_idx,bertCol1].item())\n",
    "                if all(row_gen[col] == df_gt.loc[best_mat_idx, col] for col in exactCol) and nam1 > 0.5 and nam > 0.5:\n",
    "                    R_m += 1\n",
    "                    print(f\"{idx_gen} - {row_gen[bertCol].item()} - {row_gen['Owner']} / {best_mat_idx} - {df_gt.loc[best_mat_idx, bertCol].item()} - {df_gt.loc[best_mat_idx, 'Owner']} ---------- {max_sim:.2f}\")\n",
    "                    #print(df_gt.loc[best_mat_idx, 'Owner'])\n",
    "                    used_gt.add(best_mat_idx)\n",
    "                    print(used_gt)\n",
    "                    unmatched_gen.discard(idx_gen)\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                if all(row_gen[col] == df_gt.loc[best_mat_idx, col] for col in exactCol):\n",
    "                    R_m += 1\n",
    "                    used_gt.add(best_mat_idx)\n",
    "                    unmatched_gen.discard(idx_gen)\n",
    "                    print(f\"{row_gen[bertCol].item()} / {df_gt.loc[best_mat_idx,bertCol].item()} ---- {max_sim}\")\n",
    "\n",
    "    R_s = len(unmatched_gen)\n",
    "    total_score = (R_m/R_t)*100\n",
    "    return {'R_t': R_t, 'R_m': R_m, 'R_s': R_s, 'R_n': R_n,  'Total_Score': total_score}"
   ],
   "id": "62f0360a37b842a1",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T18:26:01.820961Z",
     "start_time": "2024-12-22T18:25:55.910141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "toMatch=\"RAID Type\"\n",
    "df_gen = raid_table(extract_tables(ppt2_path, toMatch))\n",
    "df_gt = raid_table(extract_tables(ppt1_path, toMatch))\n",
    "exactCol = ['RAID Type', 'Due Date', 'Status']\n",
    "bertCol = ['RAID Description']\n",
    "bertCol1 = ['Mitigation']\n",
    "Score = eval_df(df_gt, df_gen, bertCol, bertCol1, exactCol, 0.5)['Total_Score']\n",
    "#print(eval_df(df_gt, df_gen, bertCol, bertCol1, exactCol, 0.5))\n",
    "print (f\"Cosine Similarity Score for the Section - {toMatch}: {Score: .2f}%\")"
   ],
   "id": "d02358f71e3d8afb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 - The decision has been made to use standard transactions and roles provided by SAP as a reference, and customize them according to the client's requirements. This decision is based on the flexibility offered by SAP Charm, which allows for customization to meet specific client needs. - Project Team / 5 - Use standard SAP transactions and roles - Project Team ---------- 0.68\n",
      "{5}\n",
      "6 - There is a risk of changes not reflecting as expected during the testing phase in the quality system. This could impact the project timeline and quality if not addressed promptly. - Tester / 0 - The risks of changes not reflecting in the test phase which can reduce the standards. - Tester ---------- 0.77\n",
      "{0, 5}\n",
      "14 - The decision to postpone the continuation of the discussion to the next day has been made due to time constraints. - Mina / 6 - Training postpone to next day - Avina ---------- 0.60\n",
      "{0, 5, 6}\n",
      "Cosine Similarity Score for the Section - RAID Type:  42.86%\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T19:24:05.304318Z",
     "start_time": "2024-12-22T19:24:03.698513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "toMatch=\"Upcoming Milestone / Work Products\"\n",
    "df_gen = milestone_table(extract_tables(ppt2_path, toMatch))\n",
    "df_gt = milestone_table(extract_tables(ppt1_path, toMatch))\n",
    "print(df_gen)\n",
    "print('--------------------')\n",
    "print(df_gt)\n",
    "exactCol = ['Due Date', 'Status']\n",
    "bertCol = ['Milestone']\n",
    "Score = eval_df(df_gt, df_gen, bertCol, None, exactCol, 0.5)['Total_Score']\n",
    "print(eval_df(df_gt, df_gen, bertCol, None, exactCol, 0.5))\n",
    "print (f\"Cosine Similarity Score for the Section - {toMatch}: {Score: .2f}%\")"
   ],
   "id": "9b7391e020e05e91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Milestone    Due Date       Status\n",
      "0                        Creation of demo ICS invite  12/11/2024  Not Started\n",
      "1             Generation of VTT file for the meeting  12/11/2024  Not Started\n",
      "2                              Creation of test data  12/11/2024     On-Track\n",
      "3                Upload of input files to SharePoint  12/11/2024     On-Track\n",
      "4  Follow-up with Nirusha regarding project visib...         TBD  Not Started\n",
      "5  Follow-up with Arlene and Hardik regarding pro...         TBD  Not Started\n",
      "--------------------\n",
      "                              Milestone    Due Date       Status\n",
      "0   WSR and RAID for Existing Recording  12/11/2024  Not Started\n",
      "1  MOM , RAID and WSR for New recording  12/11/2024  Not Started\n",
      "{'R_t': 2, 'R_m': 0, 'R_s': 6, 'R_n': 6, 'Total_Score': 0.0}\n",
      "Cosine Similarity Score for the Section - Upcoming Milestone / Work Products:  0.00%\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65e3e6bf4dff9295"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
